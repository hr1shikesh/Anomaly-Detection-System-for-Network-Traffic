from scipy.io import arff
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
import time





train_path = 'KDDTrain+.arff'
test_path = 'KDDTest+.arff'

train_arff = arff.loadarff(train_path)
test_arff = arff.loadarff(test_path)

train_df = pd.DataFrame(train_arff[0])
test_df = pd.DataFrame(test_arff[0])

for col in train_df.columns:
  if train_df[col].dtype == 'object':
    train_df[col] = train_df[col].str.decode('utf-8')

for col in test_df.columns:
  if test_df[col].dtype == 'object':
    test_df[col] = test_df[col].str.decode('utf-8')

train_df.head()


# Save train_df to CSV
train_csv_path = 'KDDTrain+.csv'
train_df.to_csv(train_csv_path, index=False)

# Save test_df to CSV
test_csv_path = 'KDDTest+.csv'
test_df.to_csv(test_csv_path, index=False)

print("DataFrames saved as CSV files.")





def preprocess_data(X_train, X_test):
    categorical_features = ['protocol_type', 'service', 'flag']
    numerical_features = X_train.drop(columns=categorical_features).columns

    # Preprocessor for handling both categorical and numerical features
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ]
    )

    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)

    return X_train_processed, X_test_processed

X = train_df.drop('class', axis=1)
y = train_df['class']

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

x_train, x_test = preprocess_data(x_train, x_test)

y_train = LabelEncoder().fit_transform(y_train)
y_test = LabelEncoder().fit_transform(y_test)






# Defining models
models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(),
    "AdaBoost": AdaBoostClassifier()
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    start_time = time.time()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    end_time = time.time()
    elapsed_time = end_time - start_time
    results[name] = {
        "Accuracy": accuracy,
        "Precision": report['weighted avg']['precision'],
        "Recall": report['weighted avg']['recall'],
        "F1-Score": report['weighted avg']['f1-score'],
        "Computation Time": elapsed_time
    }

# Display results
for model_name, metrics in results.items():
    print(f"{model_name}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")






from scipy.io import arff
import pandas as pd

# Load the dataset
train_path = 'KDDTrain+.arff'
test_path = 'KDDTest+.arff'

train_arff = arff.loadarff(train_path)
test_arff = arff.loadarff(test_path)

train_df = pd.DataFrame(train_arff[0])
test_df = pd.DataFrame(test_arff[0])

# Decode bytes to strings for categorical columns
for col in train_df.columns:
    if train_df[col].dtype == 'object':
        train_df[col] = train_df[col].str.decode('utf-8')

for col in test_df.columns:
    if test_df[col].dtype == 'object':
        test_df[col] = test_df[col].str.decode('utf-8')

# Apply attack labeling function
def label_attack_type(row):
    # Attack labeling rules (as provided previously)
    if row['protocol_type'] == 'icmp' and row['service'] in ['eco_i', 'ecr_i'] and row['flag'] == 'SF' and row['dst_host_same_srv_rate'] > 0.5:
        return 'Smurf'
    elif row['protocol_type'] == 'tcp' and row['flag'] == 'S0' and (row['serror_rate'] > 0.5 or row['srv_serror_rate'] > 0.5):
        return 'Neptune'
    elif row['protocol_type'] == 'tcp' and row['diff_srv_rate'] > 0.5 and row['flag'] in ['SF', 'S1']:
        return 'Nmap'
    elif row['srv_count'] > 10 and row['dst_host_srv_count'] > 10 and row['protocol_type'] in ['tcp', 'icmp'] and row['flag'] in ['SF', 'S1']:
        return 'Satan'
    elif row['protocol_type'] == 'tcp' and row['service'] == 'private' and row['dst_host_count'] > 200 and row['flag'] == 'REJ':
        return 'Portsweep'
    elif row['protocol_type'] == 'icmp' and row['service'] == 'other' and row['serror_rate'] < 0.1:
        return 'Ping_of_Death'
    elif row['service'] in ['ftp', 'telnet'] and row['num_failed_logins'] > 0 and row['protocol_type'] == 'tcp':
        return 'Guess_password'
    elif row['service'] == 'ftp' and row['num_file_creations'] > 0 and row['num_failed_logins'] > 0:
        return 'FTP_write'
    elif row['root_shell'] > 0 or row['num_root'] > 0 or row['num_shells'] > 0 or row['su_attempted'] > 0:
        return 'Rootkit'
    elif (row['src_bytes'] > 1000 or row['dst_bytes'] > 1000) and row['flag'] in ['S0', 'REJ'] and (row['serror_rate'] > 0.5 or row['dst_host_serror_rate'] > 0.5):
        return 'DoS'
    elif (row['count'] > 10 or row['srv_count'] > 10) and row['flag'] in ['S1', 'S3', 'SF'] and row['protocol_type'] in ['tcp', 'icmp']:
        return 'Probe'
    elif (row['num_failed_logins'] > 0 or row['is_guest_login'] == '1') and row['service'] in ['ftp', 'telnet', 'ssh'] and (row['num_access_files'] > 0 or row['num_file_creations'] > 0):
        return 'R2L'
    elif (row['root_shell'] > 0 or row['su_attempted'] > 0) and (row['num_root'] > 0 or row['num_file_creations'] > 0 or row['num_shells'] > 0):
        return 'U2R'
    else:
        return 'Other'

# Apply the labeling function to training and test datasets
train_df['attack_type'] = train_df.apply(lambda row: label_attack_type(row) if row['class'] == 'anomaly' else 'normal', axis=1)
test_df['attack_type'] = test_df.apply(lambda row: label_attack_type(row) if row['class'] == 'anomaly' else 'normal', axis=1)

# Encode categorical columns
categorical_columns = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_host_login', 'is_guest_login']
train_df_encoded = pd.get_dummies(train_df, columns=categorical_columns)
test_df_encoded = pd.get_dummies(test_df, columns=categorical_columns)

# Align columns between train and test sets
train_df_encoded, test_df_encoded = train_df_encoded.align(test_df_encoded, join='left', axis=1, fill_value=0)


from sklearn.model_selection import train_test_split

# Prepare features and labels
X = train_df_encoded.drop(columns=['class', 'attack_type'])
y = train_df_encoded['class'].apply(lambda x: 0 if x == 'normal' else 1)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Prepare test data
X_test = test_df_encoded.drop(columns=['class', 'attack_type'])
y_test = test_df_encoded['class'].apply(lambda x: 0 if x == 'normal' else 1)

# Display shapes of datasets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)


from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Initialize and train the XGBoost model without tuning
model = XGBClassifier(eval_metric='logloss')
model.fit(X_train, y_train)

# Predict on validation set
y_val_pred = model.predict(X_val)

# Evaluate on validation set
print("Validation Accuracy:", accuracy_score(y_val, y_val_pred))
print("Validation Classification Report:\n", classification_report(y_val, y_val_pred))
print("Validation Confusion Matrix:\n", confusion_matrix(y_val, y_val_pred))

# Predict on test set
y_test_pred = model.predict(X_test)

# Evaluate on test set
print("Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("Test Classification Report:\n", classification_report(y_test, y_test_pred))
print("Test Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))



from sklearn.model_selection import StratifiedKFold, GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define parameter grid with regularization
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100],
    'subsample': [0.8, 1.0],
    'reg_alpha': [0, 0.1, 0.5],  # L1 regularization
    'reg_lambda': [1, 1.5, 2]    # L2 regularization
}

# Calculate scale_pos_weight for class imbalance
scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])

# Set up GridSearchCV with StratifiedKFold and scale_pos_weight
grid_search = GridSearchCV(
    estimator=XGBClassifier(eval_metric='logloss', scale_pos_weight=scale_pos_weight),
    param_grid=param_grid,
    scoring='f1',
    cv=StratifiedKFold(n_splits=3),
    verbose=1,
    n_jobs=-1
)

# Run grid search
grid_search.fit(X_train, y_train)

# Best parameters and score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best F1 Score on Training Set:", best_score)

# Extract best parameters
best_model = XGBClassifier(
    learning_rate=best_params['learning_rate'],
    max_depth=best_params['max_depth'],
    n_estimators=best_params['n_estimators'],
    subsample=best_params['subsample'],
    reg_alpha=best_params['reg_alpha'],
    reg_lambda=best_params['reg_lambda'],
    eval_metric='logloss',
    scale_pos_weight=scale_pos_weight
)

# Train final model with early stopping
best_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=True)

# Evaluate on validation set
y_val_pred_tuned = best_model.predict(X_val)
print("Tuned Model Validation Accuracy:", accuracy_score(y_val, y_val_pred_tuned))
print("Tuned Model Validation Classification Report:\n", classification_report(y_val, y_val_pred_tuned))

# Evaluate on test set
y_test_pred_tuned = best_model.predict(X_test)
print("Tuned Model Test Accuracy:", accuracy_score(y_test, y_test_pred_tuned))
print("Tuned Model Test Classification Report:\n", classification_report(y_test, y_test_pred_tuned))



# Save the trained model to a file
best_model.save_model("best_xgboost_model.json")


import shap
import matplotlib.pyplot as plt

# Initialize SHAP explainer
explainer = shap.TreeExplainer(best_model)

def predict_instance(instance_index):
    """
    Function to predict and analyze a single instance from X_test using SHAP.
    
    Parameters:
    - instance_index: Index of the instance in X_test to analyze.
    """
    # Select the instance
    instance = X_test.iloc[instance_index:instance_index+1]
    
    # Predict the class (0: normal, 1: anomaly)
    prediction = best_model.predict(instance)[0]
    print(f"Prediction for instance {instance_index}: {'Anomaly' if prediction == 1 else 'Normal'}")
    
    # Identify the attack type if it's an anomaly
    if prediction == 1:
        attack_type = label_attack_type(test_df.iloc[instance_index])
        print(f"Identified Attack Type: {attack_type}")
    else:
        attack_type = 'Normal'
    
    # Calculate SHAP values for the instance
    shap_values = explainer.shap_values(instance)
    
    # Generate SHAP force plot for the instance
    print(f"Generating SHAP Force Plot for Instance {instance_index}...")
    shap.force_plot(explainer.expected_value, shap_values, instance, matplotlib=True)
    plt.show()
    
    # Generate SHAP waterfall plot for the instance
    print(f"Generating SHAP Waterfall Plot for Instance {instance_index}...")
    shap.waterfall_plot(shap.Explanation(values=shap_values[0], base_values=explainer.expected_value, data=instance.iloc[0]))
    plt.show()

# Example usage
# To analyze a specific instance in X_test, call `predict_instance(instance_index)`
predict_instance(40)  # Change 0 to any index within X_test to analyze different instances



import shap
import pandas as pd
import matplotlib.pyplot as plt

# Initialize SHAP explainer
explainer = shap.TreeExplainer(best_model)

# Function to get top N important features based on SHAP values for each instance
def get_top_features(shap_values, feature_names, top_n=5):
    # Get the absolute SHAP values
    shap_abs = pd.DataFrame(shap_values, columns=feature_names).abs()
    # Get the top N features by SHAP value for each instance
    top_features = shap_abs.apply(lambda row: row.nlargest(top_n).index.tolist(), axis=1)
    return top_features

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# Get the top 5 features for each instance
top_features = get_top_features(shap_values, X_test.columns, top_n=5)

# Add the top features as a new column to the test DataFrame for easy grouping
test_df['top_features'] = top_features

# Aggregate results by anomaly type
top_features_by_type = test_df.groupby('attack_type')['top_features'].apply(lambda x: x.explode().value_counts().head(5))

# Display the top features for each attack type
print("Top features by attack type:")
print(top_features_by_type)

# Save the results to a CSV file for further analysis if needed
top_features_by_type.to_csv("top_features_by_attack_type.csv")




